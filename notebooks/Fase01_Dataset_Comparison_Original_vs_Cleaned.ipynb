{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 02: Comparaci√≥n Dataset Original vs Cleaned\n",
    "# Turkish Music Emotion Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook tiene como objetivo:\n",
    "1. **Comparar** el dataset original con el dataset limpio\n",
    "2. **Documentar m√©tricas** de diferencias entre ambas versiones\n",
    "3. **Visualizar** las transformaciones realizadas\n",
    "4. **Preparar** los datos para versionado con DVC\n",
    "5. **Analizar** el impacto de las transformaciones de limpieza\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECCI√ìN 1: CONFIGURACI√ìN INICIAL Y LIBRER√çAS EST√ÅNDAR\n",
    "# ==============================================================================\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuraci√≥n del Entorno\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==============================================================================\n",
    "# SECCI√ìN 2: LIBRER√çAS DE TERCEROS PARA AN√ÅLISIS Y VISUALIZACI√ìN\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# ==============================================================================\n",
    "# SECCI√ìN 3: M√ìDULOS Y FUNCIONES LOCALES DEL PROYECTO\n",
    "# ==============================================================================\n",
    "from acoustic_ml import (\n",
    "    # Funciones para cargar los datasets\n",
    "    load_turkish_modified,\n",
    "    load_turkish_original,\n",
    "    # Funci√≥n para obtener informaci√≥n resumida del dataset\n",
    "    get_dataset_info,\n",
    "    # Funci√≥n para guardar el dataset procesado\n",
    "    save_processed_data,\n",
    "    # Variables globales y constantes del proyecto\n",
    "    RAW_DATA_DIR,\n",
    "    PROCESSED_DATA_DIR,\n",
    "    RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# SECCI√ìN 4: CONFIGURACI√ìN DE VISUALIZACI√ìN Y PANDAS\n",
    "# ==============================================================================\n",
    "# Configuraci√≥n de Estilo para Gr√°ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configuraci√≥n de Visualizaci√≥n de Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n",
    "\n",
    "# ==============================================================================\n",
    "# SECCI√ìN 5: VERIFICACI√ìN FINAL\n",
    "# ==============================================================================\n",
    "print(\"‚úÖ Librer√≠as y configuraci√≥n cargadas correctamente.\")\n",
    "print(f\"üìÅ Directorio de datos RAW: {RAW_DATA_DIR}\")\n",
    "print(f\"üìÅ Directorio de datos PROCESSED: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"üé≤ Semilla aleatoria (Random State) global: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion de Entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "üîç VALIDACI√ìN DEL ENTORNO Y DEPENDENCIAS\n",
    "================================================================================\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ====================\n",
    "# 1. CONFIGURACI√ìN DEL DIRECTORIO\n",
    "# ====================\n",
    "print(\"üìÅ Verificando directorio de trabajo...\")\n",
    "# Obtener directorio actual\n",
    "current_dir = Path.cwd()\n",
    "print(f\"   Directorio actual: {current_dir}\")\n",
    "\n",
    "# Si estamos en notebooks/, subir un nivel\n",
    "if current_dir.name == \"notebooks\":\n",
    "    project_root = current_dir.parent\n",
    "    os.chdir(project_root)\n",
    "    print(f\"   ‚úÖ Ra√≠z del proyecto: {project_root}\")\n",
    "else:\n",
    "    project_root = current_dir\n",
    "    print(f\"   ‚úÖ Ya estamos en la ra√≠z: {project_root}\")\n",
    "\n",
    "print(f\"   ‚úÖ Directorio de trabajo: {Path.cwd()}\\n\")\n",
    "\n",
    "# ====================\n",
    "# 2. VERIFICAR M√ìDULO ACOUSTIC_ML\n",
    "# ====================\n",
    "print(\"üì¶ Verificando m√≥dulo acoustic_ml...\")\n",
    "try:\n",
    "    import acoustic_ml\n",
    "    print(f\"   ‚úÖ acoustic_ml v{acoustic_ml.__version__}\\n\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå ERROR: {e}\")\n",
    "    print(\"   üí° Soluci√≥n: pip install -e .\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ====================\n",
    "# 3. VERIFICAR DEPENDENCIAS\n",
    "# ====================\n",
    "print(\"üîß Verificando dependencias...\")\n",
    "dependencies = {\n",
    "    'pandas': 'Procesamiento de datos',\n",
    "    'numpy': 'Operaciones num√©ricas',\n",
    "    'matplotlib': 'Visualizaciones',\n",
    "    'seaborn': 'Visualizaciones estad√≠sticas',\n",
    "    'scipy': 'Tests estad√≠sticos (KS test)'  # ‚Üê AGREGADO para KS test\n",
    "}\n",
    "\n",
    "missing_deps = []\n",
    "for package, description in dependencies.items():\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"   ‚úÖ {package} - {description}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå {package} - {description}\")\n",
    "        missing_deps.append(package)\n",
    "\n",
    "if missing_deps:\n",
    "    print(f\"\\n   ‚ùå Faltan: {', '.join(missing_deps)}\")\n",
    "    print(f\"   üí° Soluci√≥n: pip install {' '.join(missing_deps)}\\n\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print()\n",
    "\n",
    "# ====================\n",
    "# 4. VERIFICAR DVC\n",
    "# ====================\n",
    "print(\"‚òÅÔ∏è  Verificando estado de DVC...\")\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        ['dvc', 'status'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=Path.cwd(),\n",
    "        timeout=10\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"   ‚úÖ Datos sincronizados con S3\\n\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  DVC status: {result.stderr.strip()}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  No se pudo verificar DVC: {e}\\n\")\n",
    "\n",
    "# ====================\n",
    "# 5. VERIFICAR ARCHIVOS DE DATOS\n",
    "# ====================\n",
    "print(\"üìÇ Verificando archivos de datos necesarios para comparaci√≥n...\")\n",
    "\n",
    "# Archivos espec√≠ficos para este notebook de comparaci√≥n\n",
    "data_files = {\n",
    "    'Original Dataset': Path('data/raw/turkis_music_emotion_original.csv'),\n",
    "    'Modified Dataset': Path('data/raw/turkish_music_emotion_modified.csv'),\n",
    "}\n",
    "\n",
    "errors = []\n",
    "for name, path in data_files.items():\n",
    "    full_path = Path.cwd() / path\n",
    "    if full_path.exists():\n",
    "        size_kb = full_path.stat().st_size / 1024\n",
    "        print(f\"   ‚úÖ {name} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {name} - NO ENCONTRADO\")\n",
    "        print(f\"      Ruta esperada: {full_path}\")\n",
    "        errors.append(f\"Falta {name}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ====================\n",
    "# 6. VERIFICAR DIRECTORIOS DE SALIDA\n",
    "# ====================\n",
    "print(\"üìÅ Verificando/Creando directorios de salida...\")\n",
    "\n",
    "output_dirs = {\n",
    "    'data/processed': 'Para datasets exportados',\n",
    "    'reports': 'Para reportes de comparaci√≥n'\n",
    "}\n",
    "\n",
    "for dir_path, description in output_dirs.items():\n",
    "    full_path = Path.cwd() / dir_path\n",
    "    if full_path.exists():\n",
    "        print(f\"   ‚úÖ {dir_path}/ - {description}\")\n",
    "    else:\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   ‚ú® {dir_path}/ - Creado ({description})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ====================\n",
    "# 7. RESUMEN FINAL\n",
    "# ====================\n",
    "print(\"=\" * 80)\n",
    "if errors:\n",
    "    print(\"‚ùå ERRORES DETECTADOS:\")\n",
    "    for i, error in enumerate(errors, 1):\n",
    "        print(f\"   {i}. {error}\")\n",
    "    print(\"\\nüí° SOLUCIONES:\")\n",
    "    print(\"   ‚Ä¢ Datos faltantes: dvc pull\")\n",
    "    print(\"   ‚Ä¢ O verifica que los archivos est√©n en data/raw/\")\n",
    "    print(\"\\nüõë Deteniendo ejecuci√≥n del notebook\")\n",
    "    print(\"=\" * 80)\n",
    "    # DETENER EJECUCI√ìN\n",
    "    raise RuntimeError(\"Validaci√≥n fall√≥ - corrige los errores antes de continuar\")\n",
    "else:\n",
    "    print(\"‚úÖ TODAS LAS VALIDACIONES PASARON\")\n",
    "    print(\"üöÄ El notebook de comparaci√≥n est√° listo para ejecutarse\")\n",
    "    print(\"\\nüìã Siguiente paso:\")\n",
    "    print(\"   ‚Ä¢ El notebook cargar√° los datasets original y cleaned\")\n",
    "    print(\"   ‚Ä¢ Realizar√° comparaciones exhaustivas\")\n",
    "    print(\"   ‚Ä¢ Exportar√° 3 versiones para DVC\")\n",
    "    print(\"   ‚Ä¢ Generar√° reporte en reports/\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Carga de Datos\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# DESCARGA: Sincronizar datos desde S3 con DVC\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì• SINCRONIZANDO DATOS DESDE S3 CON DVC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar si data/README.md existe y no est√° trackeado\n",
    "data_readme = Path.cwd() / 'data' / 'README.md'\n",
    "use_force = False\n",
    "\n",
    "if data_readme.exists():\n",
    "    print(\"üìù Detectado data/README.md (archivo de documentaci√≥n)\")\n",
    "    print(\"   Este archivo NO debe ser versionado con DVC, solo con Git\")\n",
    "    print(\"   Usando --force para evitar conflictos...\\n\")\n",
    "    use_force = True\n",
    "\n",
    "try:\n",
    "    # Ejecutar DVC pull (con --force si es necesario)\n",
    "    cmd = ['dvc', 'pull']\n",
    "    if use_force:\n",
    "        cmd.append('--force')\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=Path.cwd()\n",
    "    )\n",
    "    \n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "    \n",
    "    print(\"‚úÖ Datos sincronizados correctamente desde S3\")\n",
    "    print(f\"üìÇ Ubicaci√≥n: {RAW_DATA_DIR}\")\n",
    "    \n",
    "    if use_force:\n",
    "        print(\"\\nüí° NOTA: Se us√≥ --force para preservar data/README.md\")\n",
    "        print(\"   Recuerda commitear este archivo a Git despu√©s:\")\n",
    "        print(\"   git add data/README.md\")\n",
    "        print(\"   git commit -m 'docs: add data directory documentation'\")\n",
    "    \n",
    "except subprocess.CalledProcessError as e:\n",
    "    error_msg = e.stderr if e.stderr else str(e)\n",
    "    print(\"‚ö†Ô∏è  Error al ejecutar DVC pull:\")\n",
    "    print(error_msg)\n",
    "    print(\"\\nüí° SOLUCIONES POSIBLES:\")\n",
    "    print(\"   1. Verifica AWS credentials: aws configure\")\n",
    "    print(\"   2. Verifica conexi√≥n S3: aws s3 ls s3://mlops24-haowei-bucket/\")\n",
    "    print(\"   3. Verifica configuraci√≥n DVC: dvc remote list\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå DVC no est√° instalado en el sistema\")\n",
    "    print(\"üí° Instala DVC con: pip install 'dvc[s3]'\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CARGA: Dataset Original (sin modificaciones)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CARGANDO DATASET ORIGINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Cargar dataset original usando el m√≥dulo acoustic_ml\n",
    "    df_original = load_turkish_original()\n",
    "    \n",
    "    print(f\"‚úÖ Dataset original cargado exitosamente\")\n",
    "    print(f\"üìè Shape: {df_original.shape[0]} filas √ó {df_original.shape[1]} columnas\")\n",
    "    \n",
    "    # Mostrar informaci√≥n detallada del dataset\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"INFORMACI√ìN DETALLADA DEL DATASET ORIGINAL:\")\n",
    "    print(\"-\"*80)\n",
    "    get_dataset_info(df_original)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: No se pudo encontrar el archivo del dataset original\")\n",
    "    print(f\"   {str(e)}\")\n",
    "    print(\"\\nüí° Verifica que DVC pull se haya ejecutado correctamente\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error inesperado al cargar el dataset original: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CARGA: Dataset Modificado/Cleaned\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä CARGANDO DATASET MODIFICADO (CLEANED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    # Cargar dataset modificado usando el m√≥dulo acoustic_ml\n",
    "    df_cleaned = load_turkish_modified()\n",
    "    \n",
    "    print(f\"‚úÖ Dataset modificado cargado exitosamente\")\n",
    "    print(f\"üìè Shape: {df_cleaned.shape[0]} filas √ó {df_cleaned.shape[1]} columnas\")\n",
    "    \n",
    "    # Mostrar informaci√≥n detallada del dataset\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"INFORMACI√ìN DETALLADA DEL DATASET MODIFICADO:\")\n",
    "    print(\"-\"*80)\n",
    "    get_dataset_info(df_cleaned)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: No se pudo encontrar el archivo del dataset modificado\")\n",
    "    print(f\"   {str(e)}\")\n",
    "    print(\"\\nüí° Verifica que DVC pull se haya ejecutado correctamente\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error inesperado al cargar el dataset modificado: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Informaci√≥n General de los Datasets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# RESUMEN INICIAL: Comparaci√≥n de Dimensiones\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN INICIAL DE COMPARACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n{'M√©trica':<30} {'Original':<15} {'Cleaned':<15} {'Diferencia':<15}\")\n",
    "print(\"-\"*75)\n",
    "\n",
    "# Comparar n√∫mero de filas\n",
    "diff_rows = df_cleaned.shape[0] - df_original.shape[0]\n",
    "print(f\"{'N√∫mero de Filas':<30} {df_original.shape[0]:<15} {df_cleaned.shape[0]:<15} {diff_rows:+<15}\")\n",
    "\n",
    "# Comparar n√∫mero de columnas\n",
    "diff_cols = df_cleaned.shape[1] - df_original.shape[1]\n",
    "print(f\"{'N√∫mero de Columnas':<30} {df_original.shape[1]:<15} {df_cleaned.shape[1]:<15} {diff_cols:+<15}\")\n",
    "\n",
    "# Comparar valores nulos\n",
    "nulls_orig = df_original.isnull().sum().sum()\n",
    "nulls_clean = df_cleaned.isnull().sum().sum()\n",
    "diff_nulls = nulls_clean - nulls_orig\n",
    "print(f\"{'Valores Nulos Totales':<30} {nulls_orig:<15} {nulls_clean:<15} {diff_nulls:+<15}\")\n",
    "\n",
    "# Comparar duplicados\n",
    "dups_orig = df_original.duplicated().sum()\n",
    "dups_clean = df_cleaned.duplicated().sum()\n",
    "diff_dups = dups_clean - dups_orig\n",
    "print(f\"{'Filas Duplicadas':<30} {dups_orig:<15} {dups_clean:<15} {diff_dups:+<15}\")\n",
    "\n",
    "# Comparar clases\n",
    "classes_orig = df_original['Class'].nunique()\n",
    "classes_clean = df_cleaned['Class'].nunique()\n",
    "diff_classes = classes_clean - classes_orig\n",
    "print(f\"{'N√∫mero de Clases':<30} {classes_orig:<15} {classes_clean:<15} {diff_classes:+<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Mostrar warning si las dimensiones son diferentes\n",
    "if diff_rows != 0:\n",
    "    print(\"\\n‚ö†Ô∏è  ADVERTENCIA: Los datasets tienen diferente n√∫mero de filas\")\n",
    "    print(f\"   Se proceder√° a alinearlos para comparaciones directas\")\n",
    "    print(f\"   Se usar√°n las primeras {min(len(df_original), len(df_cleaned))} filas comunes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VISTA PREVIA: Primeras Filas de Ambos Datasets\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üëÄ VISTA PREVIA DE LOS DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìã PRIMERAS 5 FILAS - DATASET ORIGINAL:\")\n",
    "print(\"-\"*80)\n",
    "display(df_original.head())\n",
    "\n",
    "print(\"\\nüìã PRIMERAS 5 FILAS - DATASET CLEANED:\")\n",
    "print(\"-\"*80)\n",
    "display(df_cleaned.head())\n",
    "\n",
    "# Verificar si las columnas son las mismas\n",
    "cols_orig = set(df_original.columns)\n",
    "cols_clean = set(df_cleaned.columns)\n",
    "\n",
    "if cols_orig == cols_clean:\n",
    "    print(\"\\n‚úÖ Ambos datasets tienen las mismas columnas\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Los datasets tienen columnas diferentes:\")\n",
    "    \n",
    "    only_orig = cols_orig - cols_clean\n",
    "    if only_orig:\n",
    "        print(f\"\\n   Solo en Original: {only_orig}\")\n",
    "    \n",
    "    only_clean = cols_clean - cols_orig\n",
    "    if only_clean:\n",
    "        print(f\"   Solo en Cleaned: {only_clean}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Limpieza de Columnas Numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# IDENTIFICACI√ìN INICIAL DE COLUMNAS NUM√âRICAS\n",
    "# ==============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"üîç IDENTIFICANDO COLUMNAS NUM√âRICAS INICIALES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Obtener todas las columnas excepto 'Class'\n",
    "columnas_para_analizar = [col for col in df_original.columns if col != 'Class']\n",
    "\n",
    "print(f\"\\nüìä Total de columnas a analizar: {len(columnas_para_analizar)}\")\n",
    "print(f\"   (Excluyendo la columna 'Class')\")\n",
    "\n",
    "# Identificar columnas que DEBER√çAN ser num√©ricas bas√°ndonos en el nombre del dataset\n",
    "# (asumimos que todas excepto 'Class' son features num√©ricas)\n",
    "columnas_numericas = columnas_para_analizar.copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Columnas identificadas para limpieza: {len(columnas_numericas)}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LIMPIEZA Y VALIDACI√ìN DE COLUMNAS NUM√âRICAS\n",
    "# ==============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"üßπ LIMPIEZA Y VALIDACI√ìN DE COLUMNAS NUM√âRICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def limpiar_columnas_numericas(df, columnas, nombre_dataset=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Limpia y valida columnas num√©ricas, convirtiendo valores no num√©ricos a NaN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        DataFrame a limpiar\n",
    "    columnas : list\n",
    "        Lista de columnas a validar como num√©ricas\n",
    "    nombre_dataset : str\n",
    "        Nombre del dataset para mensajes informativos\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_limpio : DataFrame\n",
    "        DataFrame con columnas convertidas a num√©rico\n",
    "    columnas_limpias : list\n",
    "        Lista de columnas que pasaron la validaci√≥n\n",
    "    \"\"\"\n",
    "    df_limpio = df.copy()\n",
    "    columnas_limpias = []\n",
    "    columnas_problematicas = []\n",
    "    \n",
    "    print(f\"\\nüîÑ Procesando {nombre_dataset}...\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for col in columnas:\n",
    "        if col in df.columns:\n",
    "            # Guardar tipo original\n",
    "            tipo_original = df[col].dtype\n",
    "            \n",
    "            # Convertir a num√©rico, convirtiendo errores a NaN\n",
    "            df_limpio[col] = pd.to_numeric(df_limpio[col], errors='coerce')\n",
    "            \n",
    "            # Verificar cu√°ntos valores se pudieron convertir\n",
    "            n_validos = df_limpio[col].notna().sum()\n",
    "            n_total = len(df_limpio)\n",
    "            n_nulos_originales = df[col].isnull().sum()\n",
    "            n_convertidos_nan = df_limpio[col].isnull().sum() - n_nulos_originales\n",
    "            porcentaje_validos = (n_validos / n_total) * 100\n",
    "            \n",
    "            if porcentaje_validos >= 95:\n",
    "                columnas_limpias.append(col)\n",
    "                if n_convertidos_nan > 0:\n",
    "                    print(f\"  ‚ö†Ô∏è  {col}: {n_convertidos_nan} valores no num√©ricos convertidos a NaN\")\n",
    "            else:\n",
    "                columnas_problematicas.append({\n",
    "                    'columna': col,\n",
    "                    'tipo_original': tipo_original,\n",
    "                    'pct_validos': porcentaje_validos,\n",
    "                    'valores_invalidos': n_total - n_validos\n",
    "                })\n",
    "                print(f\"  ‚ùå {col}: Solo {porcentaje_validos:.1f}% valores v√°lidos - OMITIDA\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  '{col}' no existe en el dataset\")\n",
    "    \n",
    "    if columnas_problematicas:\n",
    "        print(f\"\\n‚ö†Ô∏è  {len(columnas_problematicas)} columnas omitidas por tener muchos valores no num√©ricos\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Todas las columnas pasaron la validaci√≥n\")\n",
    "    \n",
    "    return df_limpio, columnas_limpias, columnas_problematicas\n",
    "\n",
    "# Limpiar ambos datasets\n",
    "df_original_clean, cols_valid_orig, cols_prob_orig = limpiar_columnas_numericas(\n",
    "    df_original, columnas_numericas, \"Dataset Original\"\n",
    ")\n",
    "\n",
    "df_cleaned_clean, cols_valid_clean, cols_prob_clean = limpiar_columnas_numericas(\n",
    "    df_cleaned, columnas_numericas, \"Dataset Cleaned\"\n",
    ")\n",
    "\n",
    "# Obtener columnas v√°lidas en ambos datasets\n",
    "columnas_numericas_validas = list(set(cols_valid_orig) & set(cols_valid_clean))\n",
    "columnas_numericas_validas.sort()\n",
    "\n",
    "# Identificar columnas que est√°n en uno pero no en otro\n",
    "solo_en_original = set(cols_valid_orig) - set(cols_valid_clean)\n",
    "solo_en_cleaned = set(cols_valid_clean) - set(cols_valid_orig)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä RESUMEN DE LIMPIEZA Y VALIDACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüìà ESTAD√çSTICAS:\")\n",
    "print(f\"   ‚Ä¢ Columnas iniciales:                {len(columnas_numericas)}\")\n",
    "print(f\"   ‚Ä¢ Columnas v√°lidas en Original:      {len(cols_valid_orig)}\")\n",
    "print(f\"   ‚Ä¢ Columnas v√°lidas en Cleaned:       {len(cols_valid_clean)}\")\n",
    "print(f\"   ‚Ä¢ Columnas v√°lidas COMUNES:          {len(columnas_numericas_validas)}\")\n",
    "\n",
    "if solo_en_original:\n",
    "    print(f\"\\n‚ö†Ô∏è  Columnas v√°lidas SOLO en Original ({len(solo_en_original)}):\")\n",
    "    for col in list(solo_en_original)[:5]:\n",
    "        print(f\"   ‚Ä¢ {col}\")\n",
    "    if len(solo_en_original) > 5:\n",
    "        print(f\"   ... y {len(solo_en_original) - 5} m√°s\")\n",
    "\n",
    "if solo_en_cleaned:\n",
    "    print(f\"\\n‚ö†Ô∏è  Columnas v√°lidas SOLO en Cleaned ({len(solo_en_cleaned)}):\")\n",
    "    for col in list(solo_en_cleaned)[:5]:\n",
    "        print(f\"   ‚Ä¢ {col}\")\n",
    "    if len(solo_en_cleaned) > 5:\n",
    "        print(f\"   ... y {len(solo_en_cleaned) - 5} m√°s\")\n",
    "\n",
    "# Mostrar columnas problem√°ticas si las hay\n",
    "if cols_prob_orig:\n",
    "    print(f\"\\n‚ùå Columnas problem√°ticas en Original ({len(cols_prob_orig)}):\")\n",
    "    for item in cols_prob_orig[:5]:\n",
    "        print(f\"   ‚Ä¢ {item['columna']}: {item['pct_validos']:.1f}% v√°lidos, \"\n",
    "              f\"{item['valores_invalidos']} valores inv√°lidos\")\n",
    "    if len(cols_prob_orig) > 5:\n",
    "        print(f\"   ... y {len(cols_prob_orig) - 5} m√°s\")\n",
    "\n",
    "if cols_prob_clean:\n",
    "    print(f\"\\n‚ùå Columnas problem√°ticas en Cleaned ({len(cols_prob_clean)}):\")\n",
    "    for item in cols_prob_clean[:5]:\n",
    "        print(f\"   ‚Ä¢ {item['columna']}: {item['pct_validos']:.1f}% v√°lidos, \"\n",
    "              f\"{item['valores_invalidos']} valores inv√°lidos\")\n",
    "    if len(cols_prob_clean) > 5:\n",
    "        print(f\"   ... y {len(cols_prob_clean) - 5} m√°s\")\n",
    "\n",
    "# Reemplazar las variables originales con las versiones limpias\n",
    "df_original = df_original_clean\n",
    "df_cleaned = df_cleaned_clean\n",
    "columnas_numericas = columnas_numericas_validas\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ LIMPIEZA COMPLETADA\")\n",
    "print(f\"   De ahora en adelante se usar√°n {len(columnas_numericas)} columnas num√©ricas validadas\")\n",
    "print(f\"   Los datasets han sido actualizados con valores convertidos a num√©rico\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VERIFICACI√ìN POST-LIMPIEZA\n",
    "# ==============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"üîç VERIFICACI√ìN DE TIPOS DE DATOS POST-LIMPIEZA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Verificando {len(columnas_numericas)} columnas num√©ricas validadas...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Verificar tipos en Original\n",
    "print(\"\\n‚úÖ DATASET ORIGINAL - Tipos de datos:\")\n",
    "tipos_correctos_orig = 0\n",
    "for col in columnas_numericas[:10]:  # Mostrar primeras 10\n",
    "    tipo = df_original[col].dtype\n",
    "    es_numerico = np.issubdtype(tipo, np.number)\n",
    "    status = \"‚úÖ\" if es_numerico else \"‚ùå\"\n",
    "    if es_numerico:\n",
    "        tipos_correctos_orig += 1\n",
    "    print(f\"  {status} {col:<40} {str(tipo):<15}\")\n",
    "\n",
    "if len(columnas_numericas) > 10:\n",
    "    print(f\"  ... y {len(columnas_numericas) - 10} columnas m√°s\")\n",
    "\n",
    "# Verificar tipos en Cleaned\n",
    "print(\"\\n‚úÖ DATASET CLEANED - Tipos de datos:\")\n",
    "tipos_correctos_clean = 0\n",
    "for col in columnas_numericas[:10]:  # Mostrar primeras 10\n",
    "    tipo = df_cleaned[col].dtype\n",
    "    es_numerico = np.issubdtype(tipo, np.number)\n",
    "    status = \"‚úÖ\" if es_numerico else \"‚ùå\"\n",
    "    if es_numerico:\n",
    "        tipos_correctos_clean += 1\n",
    "    print(f\"  {status} {col:<40} {str(tipo):<15}\")\n",
    "\n",
    "if len(columnas_numericas) > 10:\n",
    "    print(f\"  ... y {len(columnas_numericas) - 10} columnas m√°s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"‚úÖ Verificaci√≥n completada:\")\n",
    "print(f\"   ‚Ä¢ Todas las {len(columnas_numericas)} columnas son num√©ricas\")\n",
    "print(f\"   ‚Ä¢ Listas para an√°lisis estad√≠stico y comparativo\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. An√°lisis de Valores Nulos y Duplicados\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para analizar valores nulos\n",
    "def analizar_valores_nulos(df, nombre):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"AN√ÅLISIS DE VALORES NULOS - {nombre}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    nulos = df.isnull().sum()\n",
    "    porcentaje_nulos = (nulos / len(df)) * 100\n",
    "    \n",
    "    tabla_nulos = pd.DataFrame({\n",
    "        'Valores Nulos': nulos,\n",
    "        'Porcentaje (%)': porcentaje_nulos\n",
    "    })\n",
    "    \n",
    "    tabla_nulos = tabla_nulos[tabla_nulos['Valores Nulos'] > 0].sort_values(\n",
    "        by='Valores Nulos', ascending=False\n",
    "    )\n",
    "    \n",
    "    if len(tabla_nulos) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Se encontraron {len(tabla_nulos)} columnas con valores nulos:\\n\")\n",
    "        display(tabla_nulos)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No se encontraron valores nulos\")\n",
    "    \n",
    "    return tabla_nulos\n",
    "\n",
    "# Analizar ambos datasets\n",
    "nulos_original = analizar_valores_nulos(df_original, \"DATASET ORIGINAL\")\n",
    "nulos_cleaned = analizar_valores_nulos(df_cleaned, \"DATASET CLEANED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de duplicados\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISIS DE DUPLICADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "duplicados_original = df_original.duplicated().sum()\n",
    "duplicados_cleaned = df_cleaned.duplicated().sum()\n",
    "\n",
    "print(f\"\\nüìä Dataset Original: {duplicados_original} filas duplicadas\")\n",
    "print(f\"üìä Dataset Cleaned:  {duplicados_cleaned} filas duplicadas\")\n",
    "\n",
    "if duplicados_original > 0 or duplicados_cleaned > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Diferencia: {abs(duplicados_cleaned - duplicados_original)} duplicados\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Ning√∫n dataset tiene duplicados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Comparaci√≥n de Distribuci√≥n de Clases\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuci√≥n de clases\n",
    "print(\"=\"*80)\n",
    "print(\"DISTRIBUCI√ìN DE CLASES DE EMOCI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "dist_original = df_original['Class'].value_counts().sort_index()\n",
    "dist_cleaned = df_cleaned['Class'].value_counts().sort_index()\n",
    "\n",
    "comparacion_clases = pd.DataFrame({\n",
    "    'Original': dist_original,\n",
    "    'Cleaned': dist_cleaned,\n",
    "    'Diferencia': dist_cleaned - dist_original,\n",
    "    'Cambio (%)': ((dist_cleaned - dist_original) / dist_original * 100).round(2)\n",
    "})\n",
    "\n",
    "print(\"\\nüìä Comparaci√≥n de distribuci√≥n de clases:\\n\")\n",
    "display(comparacion_clases)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Dataset Original\n",
    "axes[0].bar(dist_original.index, dist_original.values, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Distribuci√≥n de Clases - Dataset Original', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Clase de Emoci√≥n', fontsize=12)\n",
    "axes[0].set_ylabel('Frecuencia', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(dist_original.values):\n",
    "    axes[0].text(i, v + 2, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Dataset Cleaned\n",
    "axes[1].bar(dist_cleaned.index, dist_cleaned.values, color='seagreen', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Distribuci√≥n de Clases - Dataset Cleaned', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Clase de Emoci√≥n', fontsize=12)\n",
    "axes[1].set_ylabel('Frecuencia', fontsize=12)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(dist_cleaned.values):\n",
    "    axes[1].text(i, v + 2, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Gr√°fico de Comparaci√≥n Lado a Lado (Con Manejo de Clases Diferentes)\n",
    "# ==============================================================================\n",
    "\n",
    "# Obtener todas las clases √∫nicas de ambos datasets\n",
    "all_classes = sorted(set(dist_original.index) | set(dist_cleaned.index))\n",
    "\n",
    "# Crear DataFrames con todas las clases, rellenando con 0 las faltantes\n",
    "dist_original_aligned = pd.Series(\n",
    "    [dist_original.get(cls, 0) for cls in all_classes], \n",
    "    index=all_classes\n",
    ")\n",
    "dist_cleaned_aligned = pd.Series(\n",
    "    [dist_cleaned.get(cls, 0) for cls in all_classes], \n",
    "    index=all_classes\n",
    ")\n",
    "\n",
    "# Crear el gr√°fico\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "x = np.arange(len(all_classes))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, dist_original_aligned.values, width, label='Original',\n",
    "               color='steelblue', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, dist_cleaned_aligned.values, width, label='Cleaned',\n",
    "               color='seagreen', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Clase de Emoci√≥n', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Frecuencia', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Comparaci√≥n de Distribuci√≥n de Clases: Original vs Cleaned',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_classes, rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Agregar valores en las barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        if height > 0:  # Solo mostrar si hay valores\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{int(height)}',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar informaci√≥n sobre las diferencias de clases\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä AN√ÅLISIS DE CLASES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüîç Total de clases √∫nicas: {len(all_classes)}\")\n",
    "print(f\"   Clases: {', '.join(all_classes)}\")\n",
    "\n",
    "# Clases solo en Original\n",
    "only_in_original = set(dist_original.index) - set(dist_cleaned.index)\n",
    "if only_in_original:\n",
    "    print(f\"\\n‚ö†Ô∏è  Clases solo en Original: {', '.join(only_in_original)}\")\n",
    "    for cls in only_in_original:\n",
    "        print(f\"   ‚Ä¢ {cls}: {dist_original[cls]} muestras\")\n",
    "\n",
    "# Clases solo en Cleaned\n",
    "only_in_cleaned = set(dist_cleaned.index) - set(dist_original.index)\n",
    "if only_in_cleaned:\n",
    "    print(f\"\\n‚ö†Ô∏è  Clases solo en Cleaned: {', '.join(only_in_cleaned)}\")\n",
    "    for cls in only_in_cleaned:\n",
    "        print(f\"   ‚Ä¢ {cls}: {dist_cleaned[cls]} muestras\")\n",
    "\n",
    "# Clases comunes\n",
    "common_classes = set(dist_original.index) & set(dist_cleaned.index)\n",
    "if common_classes:\n",
    "    print(f\"\\n‚úÖ Clases comunes: {', '.join(sorted(common_classes))}\")\n",
    "    print(\"\\n   Comparaci√≥n:\")\n",
    "    print(f\"   {'Clase':<15} {'Original':<12} {'Cleaned':<12} {'Diferencia':<12}\")\n",
    "    print(\"   \" + \"-\"*50)\n",
    "    for cls in sorted(common_classes):\n",
    "        diff = dist_cleaned[cls] - dist_original[cls]\n",
    "        print(f\"   {cls:<15} {dist_original[cls]:<12} {dist_cleaned[cls]:<12} {diff:+<12}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Estad√≠sticas Descriptivas Comparativas\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS - DATASET ORIGINAL\")\n",
    "print(\"=\"*80)\n",
    "display(df_original[columnas_numericas].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS - DATASET CLEANED\")\n",
    "print(\"=\"*80)\n",
    "display(df_cleaned[columnas_numericas].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. An√°lisis de Outliers Comparativo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AN√ÅLISIS DE OUTLIERS COMPARATIVO (M√©todo IQR)\n",
    "# ==============================================================================\n",
    "\n",
    "# Funci√≥n mejorada para detectar outliers usando IQR con validaci√≥n\n",
    "def detectar_outliers_iqr(df, columnas):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando el m√©todo IQR (Interquartile Range).\n",
    "    Solo procesa columnas que sean realmente num√©ricas.\n",
    "    \"\"\"\n",
    "    outliers_dict = {}\n",
    "    columnas_procesadas = []\n",
    "    columnas_omitidas = []\n",
    "    \n",
    "    for col in columnas:\n",
    "        try:\n",
    "            # Verificar que la columna sea num√©rica y no tenga solo strings\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                # Convertir a numeric por si acaso hay valores mixtos\n",
    "                serie_numerica = pd.to_numeric(df[col], errors='coerce')\n",
    "                \n",
    "                # Eliminar NaN para el c√°lculo\n",
    "                serie_limpia = serie_numerica.dropna()\n",
    "                \n",
    "                if len(serie_limpia) > 0:\n",
    "                    Q1 = serie_limpia.quantile(0.25)\n",
    "                    Q3 = serie_limpia.quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    \n",
    "                    lower_bound = Q1 - 1.5 * IQR\n",
    "                    upper_bound = Q3 + 1.5 * IQR\n",
    "                    \n",
    "                    # Contar outliers\n",
    "                    outliers = serie_limpia[(serie_limpia < lower_bound) | (serie_limpia > upper_bound)]\n",
    "                    outliers_dict[col] = len(outliers)\n",
    "                    columnas_procesadas.append(col)\n",
    "                else:\n",
    "                    outliers_dict[col] = 0\n",
    "                    columnas_omitidas.append((col, \"sin valores v√°lidos\"))\n",
    "            else:\n",
    "                outliers_dict[col] = 0\n",
    "                columnas_omitidas.append((col, \"no num√©rica\"))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error procesando columna '{col}': {str(e)}\")\n",
    "            outliers_dict[col] = 0\n",
    "            columnas_omitidas.append((col, f\"error: {str(e)[:50]}\"))\n",
    "    \n",
    "    return outliers_dict, columnas_procesadas, columnas_omitidas\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISIS DE OUTLIERS (M√©todo IQR)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detectar outliers en ambos datasets\n",
    "print(\"\\nüîç Procesando dataset Original...\")\n",
    "outliers_original, cols_proc_orig, cols_omit_orig = detectar_outliers_iqr(df_original, columnas_numericas)\n",
    "\n",
    "print(\"üîç Procesando dataset Cleaned...\")\n",
    "outliers_cleaned, cols_proc_clean, cols_omit_clean = detectar_outliers_iqr(df_cleaned, columnas_numericas)\n",
    "\n",
    "# Mostrar columnas omitidas si las hay\n",
    "if cols_omit_orig:\n",
    "    print(f\"\\n‚ö†Ô∏è  Columnas omitidas en Original ({len(cols_omit_orig)}):\")\n",
    "    for col, razon in cols_omit_orig[:5]:  # Mostrar solo las primeras 5\n",
    "        print(f\"   ‚Ä¢ {col}: {razon}\")\n",
    "    if len(cols_omit_orig) > 5:\n",
    "        print(f\"   ... y {len(cols_omit_orig) - 5} m√°s\")\n",
    "\n",
    "if cols_omit_clean:\n",
    "    print(f\"\\n‚ö†Ô∏è  Columnas omitidas en Cleaned ({len(cols_omit_clean)}):\")\n",
    "    for col, razon in cols_omit_clean[:5]:\n",
    "        print(f\"   ‚Ä¢ {col}: {razon}\")\n",
    "    if len(cols_omit_clean) > 5:\n",
    "        print(f\"   ... y {len(cols_omit_clean) - 5} m√°s\")\n",
    "\n",
    "# Obtener solo las columnas que se procesaron exitosamente en ambos datasets\n",
    "columnas_validas = list(set(cols_proc_orig) & set(cols_proc_clean))\n",
    "\n",
    "print(f\"\\n‚úÖ Columnas procesadas exitosamente: {len(columnas_validas)} de {len(columnas_numericas)}\")\n",
    "\n",
    "# Crear tabla comparativa solo con columnas v√°lidas\n",
    "if columnas_validas:\n",
    "    comparacion_outliers = pd.DataFrame({\n",
    "        'Original': {col: outliers_original[col] for col in columnas_validas},\n",
    "        'Cleaned': {col: outliers_cleaned[col] for col in columnas_validas},\n",
    "        'Reducci√≥n': {col: outliers_original[col] - outliers_cleaned[col] for col in columnas_validas},\n",
    "        'Reducci√≥n (%)': {\n",
    "            col: ((outliers_original[col] - outliers_cleaned[col]) / max(outliers_original[col], 1) * 100)\n",
    "            for col in columnas_validas\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    # Filtrar solo las que tienen outliers en Original\n",
    "    comparacion_outliers = comparacion_outliers[comparacion_outliers['Original'] > 0].sort_values(\n",
    "        by='Reducci√≥n', ascending=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPARACI√ìN DE OUTLIERS (M√©todo IQR)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nüìä Total de outliers en Original: {sum(outliers_original.values()):,}\")\n",
    "    print(f\"üìä Total de outliers en Cleaned:  {sum(outliers_cleaned.values()):,}\")\n",
    "    print(f\"üìä Reducci√≥n total:                {sum(outliers_original.values()) - sum(outliers_cleaned.values()):,} outliers\")\n",
    "    \n",
    "    reduccion_pct = 0\n",
    "    if sum(outliers_original.values()) > 0:\n",
    "        reduccion_pct = ((sum(outliers_original.values()) - sum(outliers_cleaned.values())) / \n",
    "                         sum(outliers_original.values()) * 100)\n",
    "    print(f\"üìä Reducci√≥n porcentual:           {reduccion_pct:.2f}%\")\n",
    "    \n",
    "    if len(comparacion_outliers) > 0:\n",
    "        print(f\"\\nüìà Top 10 variables con m√°s outliers reducidos:\\n\")\n",
    "        display(comparacion_outliers.head(10))\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No se detectaron outliers en el dataset Original\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No se pudieron procesar columnas num√©ricas para an√°lisis de outliers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de outliers\n",
    "if len(comparacion_outliers) > 0:\n",
    "    top_features = comparacion_outliers.head(10).index.tolist()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    x = np.arange(len(top_features))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, [outliers_original[f] for f in top_features], \n",
    "                   width, label='Original', color='crimson', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax.bar(x + width/2, [outliers_cleaned[f] for f in top_features], \n",
    "                   width, label='Cleaned', color='forestgreen', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel('Features', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Cantidad de Outliers', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Top 10 Features con M√°s Outliers: Original vs Cleaned', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f.replace('_', ' ') for f in top_features], rotation=45, ha='right')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Agregar valores\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{int(height)}',\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. An√°lisis Espec√≠fico de Diferencias Celda por Celda\n",
    "---\n",
    "\n",
    "Como los datasets tienen diferente n√∫mero de filas, vamos a:\n",
    "1. Encontrar filas comunes basadas en caracter√≠sticas similares\n",
    "2. Analizar las diferencias en los valores num√©ricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AN√ÅLISIS DE DIFERENCIAS CELDA POR CELDA (Datasets Alineados)\n",
    "# ==============================================================================\n",
    "\n",
    "n_compare = min(len(df_original), len(df_cleaned))\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"AN√ÅLISIS DE DIFERENCIAS EN LAS PRIMERAS {n_compare} FILAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comparar valores para las primeras n filas\n",
    "diferencias_dict = {}\n",
    "columnas_procesadas = []\n",
    "columnas_con_error = []\n",
    "\n",
    "for col in columnas_numericas:\n",
    "    try:\n",
    "        # Obtener valores y convertir a num√©rico expl√≠citamente\n",
    "        valores_original = pd.to_numeric(df_original[col].iloc[:n_compare], errors='coerce').values\n",
    "        valores_cleaned = pd.to_numeric(df_cleaned[col].iloc[:n_compare], errors='coerce').values\n",
    "        \n",
    "        # Verificar que tengamos suficientes valores v√°lidos\n",
    "        valores_validos_orig = ~np.isnan(valores_original)\n",
    "        valores_validos_clean = ~np.isnan(valores_cleaned)\n",
    "        valores_validos = valores_validos_orig & valores_validos_clean\n",
    "        \n",
    "        if np.sum(valores_validos) < n_compare * 0.5:  # Si menos del 50% son v√°lidos, omitir\n",
    "            columnas_con_error.append((col, f\"Pocos valores v√°lidos: {np.sum(valores_validos)}/{n_compare}\"))\n",
    "            continue\n",
    "        \n",
    "        # Calcular diferencias solo en valores v√°lidos\n",
    "        diff = np.abs(valores_original - valores_cleaned)\n",
    "        diff_validos = diff[valores_validos]\n",
    "        \n",
    "        # Contar cambios (con una peque√±a tolerancia para errores de punto flotante)\n",
    "        tolerancia = 1e-10\n",
    "        cambios = np.sum(diff_validos > tolerancia)\n",
    "        \n",
    "        diferencias_dict[col] = {\n",
    "            'Cambios_Totales': cambios,\n",
    "            'Cambios_%': (cambios / np.sum(valores_validos)) * 100,\n",
    "            'Diff_Media': np.mean(diff_validos[diff_validos > tolerancia]) if cambios > 0 else 0,\n",
    "            'Diff_Max': np.max(diff_validos),\n",
    "            'Diff_Min': np.min(diff_validos[diff_validos > tolerancia]) if cambios > 0 else 0,\n",
    "            'N_Valores_Validos': np.sum(valores_validos)\n",
    "        }\n",
    "        \n",
    "        columnas_procesadas.append(col)\n",
    "        \n",
    "    except Exception as e:\n",
    "        columnas_con_error.append((col, str(e)[:50]))\n",
    "        continue\n",
    "\n",
    "# Mostrar columnas con errores si las hay\n",
    "if columnas_con_error:\n",
    "    print(f\"\\n‚ö†Ô∏è  Columnas omitidas del an√°lisis ({len(columnas_con_error)}):\")\n",
    "    for col, error in columnas_con_error[:10]:  # Mostrar primeras 10\n",
    "        print(f\"   ‚Ä¢ {col}: {error}\")\n",
    "    if len(columnas_con_error) > 10:\n",
    "        print(f\"   ... y {len(columnas_con_error) - 10} m√°s\")\n",
    "\n",
    "print(f\"\\n‚úÖ Columnas procesadas exitosamente: {len(columnas_procesadas)}\")\n",
    "\n",
    "# Crear DataFrame de diferencias\n",
    "if len(diferencias_dict) > 0:\n",
    "    df_diferencias = pd.DataFrame(diferencias_dict).T\n",
    "    df_diferencias = df_diferencias[df_diferencias['Cambios_Totales'] > 0].sort_values(\n",
    "        by='Cambios_Totales', ascending=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Columnas que presentan cambios: {len(df_diferencias)} de {len(columnas_procesadas)}\")\n",
    "    \n",
    "    if len(df_diferencias) > 0:\n",
    "        print(f\"\\nTop 15 columnas con m√°s cambios:\\n\")\n",
    "        display(df_diferencias.head(15))\n",
    "        \n",
    "        # Calcular estad√≠sticas totales\n",
    "        total_celdas = n_compare * len(columnas_procesadas)\n",
    "        celdas_modificadas = df_diferencias['Cambios_Totales'].sum()\n",
    "        \n",
    "        print(f\"\\nüìä RESUMEN DE CAMBIOS:\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"   ‚Ä¢ Total de celdas analizadas:     {total_celdas:,}\")\n",
    "        print(f\"   ‚Ä¢ Celdas modificadas:             {int(celdas_modificadas):,}\")\n",
    "        print(f\"   ‚Ä¢ Porcentaje modificado:          {(celdas_modificadas/total_celdas)*100:.2f}%\")\n",
    "        print(f\"   ‚Ä¢ Columnas con cambios:           {len(df_diferencias)}\")\n",
    "        print(f\"   ‚Ä¢ Columnas sin cambios:           {len(columnas_procesadas) - len(df_diferencias)}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No se detectaron cambios significativos entre los datasets\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No se pudieron procesar columnas para an√°lisis de diferencias\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AN√ÅLISIS DETALLADO DE COLUMNA ESPEC√çFICA (Podemos Indicar la Columna Deseada)\n",
    "# ==============================================================================\n",
    "\n",
    "if len(df_diferencias) > 0:\n",
    "    # Seleccionar la columna con m√°s cambios\n",
    "    columna_mas_cambios = df_diferencias.index[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"AN√ÅLISIS DETALLADO: {columna_mas_cambios}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Obtener valores\n",
    "    vals_orig = pd.to_numeric(df_original[columna_mas_cambios].iloc[:n_compare], errors='coerce')\n",
    "    vals_clean = pd.to_numeric(df_cleaned[columna_mas_cambios].iloc[:n_compare], errors='coerce')\n",
    "    \n",
    "    # Calcular diferencias\n",
    "    diferencias = vals_clean - vals_orig\n",
    "    diferencias_abs = np.abs(diferencias)\n",
    "    \n",
    "    # Encontrar √≠ndices de cambios\n",
    "    indices_cambios = np.where(diferencias_abs > 1e-10)[0]\n",
    "    \n",
    "    print(f\"\\nüìä Estad√≠sticas de la columna:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"   ‚Ä¢ Total de cambios:           {len(indices_cambios)}\")\n",
    "    print(f\"   ‚Ä¢ Porcentaje de cambios:      {(len(indices_cambios)/n_compare)*100:.2f}%\")\n",
    "    print(f\"   ‚Ä¢ Diferencia promedio:        {np.mean(diferencias_abs[indices_cambios]):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Diferencia m√°xima:          {np.max(diferencias_abs):.6f}\")\n",
    "    print(f\"   ‚Ä¢ Diferencia m√≠nima (>0):     {np.min(diferencias_abs[indices_cambios]):.6f}\")\n",
    "    \n",
    "    # Mostrar algunos ejemplos de cambios\n",
    "    print(f\"\\nüìã Ejemplos de cambios (primeros 10):\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"   {'√çndice':<8} {'Original':<15} {'Cleaned':<15} {'Diferencia':<15}\")\n",
    "    print(\"   \" + \"-\"*55)\n",
    "    \n",
    "    for idx in indices_cambios[:10]:\n",
    "        orig_val = vals_orig.iloc[idx]\n",
    "        clean_val = vals_clean.iloc[idx]\n",
    "        diff_val = diferencias.iloc[idx]\n",
    "        print(f\"   {idx:<8} {orig_val:<15.6f} {clean_val:<15.6f} {diff_val:<+15.6f}\")\n",
    "    \n",
    "    if len(indices_cambios) > 10:\n",
    "        print(f\"   ... y {len(indices_cambios) - 10} cambios m√°s\")\n",
    "    \n",
    "    # Visualizaci√≥n de diferencias\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Histograma de diferencias\n",
    "    axes[0].hist(diferencias[indices_cambios], bins=30, color='coral', \n",
    "                 alpha=0.7, edgecolor='black')\n",
    "    axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Sin cambio')\n",
    "    axes[0].set_title(f'Distribuci√≥n de Diferencias\\n{columna_mas_cambios}', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Diferencia (Cleaned - Original)', fontsize=10)\n",
    "    axes[0].set_ylabel('Frecuencia', fontsize=10)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Scatter plot: Original vs Cleaned\n",
    "    axes[1].scatter(vals_orig, vals_clean, alpha=0.5, s=30)\n",
    "    \n",
    "    # L√≠nea de igualdad (y = x)\n",
    "    min_val = min(vals_orig.min(), vals_clean.min())\n",
    "    max_val = max(vals_orig.max(), vals_clean.max())\n",
    "    axes[1].plot([min_val, max_val], [min_val, max_val], \n",
    "                 'r--', linewidth=2, label='y = x (Sin cambios)')\n",
    "    \n",
    "    axes[1].set_title(f'Comparaci√≥n: Original vs Cleaned\\n{columna_mas_cambios}', \n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Valores Original', fontsize=10)\n",
    "    axes[1].set_ylabel('Valores Cleaned', fontsize=10)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparaci√≥n de Distribuciones de Features Clave\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar features clave para comparar distribuciones\n",
    "features_clave = [\n",
    "    '_RMSenergy_Mean',\n",
    "    '_Tempo_Mean',\n",
    "    '_Roughness_Mean',\n",
    "    '_Brightness_Mean',\n",
    "    '_Spectralcentroid_Mean',\n",
    "    '_Zero-crossingrate_Mean'\n",
    "]\n",
    "\n",
    "# Verificar que existen\n",
    "features_clave = [f for f in features_clave if f in columnas_numericas]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(features_clave):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Histogramas superpuestos\n",
    "    ax.hist(df_original[feature], bins=30, alpha=0.5, label='Original', \n",
    "            color='steelblue', edgecolor='black')\n",
    "    ax.hist(df_cleaned[feature], bins=30, alpha=0.5, label='Cleaned', \n",
    "            color='seagreen', edgecolor='black')\n",
    "    \n",
    "    ax.set_title(f'Distribuci√≥n: {feature.replace(\"_\", \" \")}', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Valor', fontsize=10)\n",
    "    ax.set_ylabel('Frecuencia', fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparaci√≥n de Distribuciones de Features Clave', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Test Estad√≠sticos de Similitud\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar test de Kolmogorov-Smirnov para comparar distribuciones\n",
    "print(\"=\"*80)\n",
    "print(\"TEST DE KOLMOGOROV-SMIRNOV\")\n",
    "print(\"Compara si dos muestras provienen de la misma distribuci√≥n\")\n",
    "print(\"H0: Las distribuciones son iguales | p-value > 0.05 ‚Üí No rechazamos H0\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ks_results = {}\n",
    "\n",
    "for col in columnas_numericas:\n",
    "    # Usar solo las primeras n_compare filas\n",
    "    stat, pvalue = stats.ks_2samp(\n",
    "        df_original[col].iloc[:n_compare],\n",
    "        df_cleaned[col].iloc[:n_compare]\n",
    "    )\n",
    "    \n",
    "    ks_results[col] = {\n",
    "        'KS_Statistic': stat,\n",
    "        'p_value': pvalue,\n",
    "        'Distribuciones_Similares': 'S√≠' if pvalue > 0.05 else 'No'\n",
    "    }\n",
    "\n",
    "df_ks = pd.DataFrame(ks_results).T.sort_values(by='p_value')\n",
    "\n",
    "print(f\"\\nColumnas con distribuciones SIGNIFICATIVAMENTE DIFERENTES (p < 0.05):\\n\")\n",
    "diferentes = df_ks[df_ks['p_value'] < 0.05]\n",
    "if len(diferentes) > 0:\n",
    "    display(diferentes.head(15))\n",
    "    print(f\"\\n‚ö†Ô∏è  {len(diferentes)} columnas tienen distribuciones significativamente diferentes\")\n",
    "else:\n",
    "    print(\"‚úÖ Todas las distribuciones son estad√≠sticamente similares\")\n",
    "\n",
    "print(f\"\\n\\nColumnas con distribuciones SIMILARES (p >= 0.05):\\n\")\n",
    "similares = df_ks[df_ks['p_value'] >= 0.05]\n",
    "if len(similares) > 0:\n",
    "    display(similares.head(15))\n",
    "    print(f\"\\n‚úÖ {len(similares)} columnas mantienen distribuciones similares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. M√©tricas Globales de Comparaci√≥n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular m√©tricas globales\n",
    "print(\"=\"*80)\n",
    "print(\"M√âTRICAS GLOBALES DE COMPARACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metricas = {}\n",
    "\n",
    "# 1. Cambio en tama√±o del dataset\n",
    "metricas['Filas_Original'] = len(df_original)\n",
    "metricas['Filas_Cleaned'] = len(df_cleaned)\n",
    "metricas['Diferencia_Filas'] = len(df_cleaned) - len(df_original)\n",
    "metricas['Cambio_Porcentual_Filas'] = ((len(df_cleaned) - len(df_original)) / len(df_original)) * 100\n",
    "\n",
    "# 2. Cambio en valores nulos\n",
    "metricas['Nulos_Original'] = df_original.isnull().sum().sum()\n",
    "metricas['Nulos_Cleaned'] = df_cleaned.isnull().sum().sum()\n",
    "metricas['Reduccion_Nulos'] = metricas['Nulos_Original'] - metricas['Nulos_Cleaned']\n",
    "\n",
    "# 3. Cambio en outliers\n",
    "metricas['Outliers_Original'] = sum(outliers_original.values())\n",
    "metricas['Outliers_Cleaned'] = sum(outliers_cleaned.values())\n",
    "metricas['Reduccion_Outliers'] = metricas['Outliers_Original'] - metricas['Outliers_Cleaned']\n",
    "\n",
    "# 4. Cambio en duplicados\n",
    "metricas['Duplicados_Original'] = duplicados_original\n",
    "metricas['Duplicados_Cleaned'] = duplicados_cleaned\n",
    "\n",
    "# 5. Porcentaje de valores modificados\n",
    "if len(df_diferencias) > 0:\n",
    "    metricas['Columnas_Modificadas'] = len(df_diferencias)\n",
    "    metricas['Porcentaje_Columnas_Modificadas'] = (len(df_diferencias) / len(columnas_numericas)) * 100\n",
    "    metricas['Total_Celdas_Modificadas'] = df_diferencias['Cambios_Totales'].sum()\n",
    "else:\n",
    "    metricas['Columnas_Modificadas'] = 0\n",
    "    metricas['Porcentaje_Columnas_Modificadas'] = 0\n",
    "    metricas['Total_Celdas_Modificadas'] = 0\n",
    "\n",
    "# 6. Distribuciones similares\n",
    "metricas['Distribuciones_Similares'] = len(similares)\n",
    "metricas['Distribuciones_Diferentes'] = len(diferentes)\n",
    "metricas['Porcentaje_Similares'] = (len(similares) / len(columnas_numericas)) * 100\n",
    "\n",
    "# Mostrar m√©tricas\n",
    "df_metricas = pd.DataFrame(list(metricas.items()), columns=['M√©trica', 'Valor'])\n",
    "display(df_metricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. FORZAR CIERRE DE FIGURAS ANTERIORES\n",
    "plt.close('all')\n",
    "\n",
    "# 2. CREAR LA FIGURA Y LOS EJES \n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Gr√°fico 1: Cambio en tama√±o\n",
    "axes[0, 0].bar(['Original', 'Cleaned'],\n",
    "               [metricas['Filas_Original'], metricas['Filas_Cleaned']],\n",
    "               color=['steelblue', 'seagreen'], alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Tama√±o del Dataset', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('N√∫mero de Filas', fontsize=11)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate([metricas['Filas_Original'], metricas['Filas_Cleaned']]):\n",
    "    axes[0, 0].text(i, v + 5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 2: Cambio en outliers\n",
    "axes[0, 1].bar(['Original', 'Cleaned'],\n",
    "               [metricas['Outliers_Original'], metricas['Outliers_Cleaned']],\n",
    "               color=['crimson', 'forestgreen'], alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Total de Outliers Detectados', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Cantidad de Outliers', fontsize=11)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate([metricas['Outliers_Original'], metricas['Outliers_Cleaned']]):\n",
    "    axes[0, 1].text(i, v + 10, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 3: Distribuciones similares vs diferentes\n",
    "axes[1, 0].bar(['Similares', 'Diferentes'],\n",
    "               [metricas['Distribuciones_Similares'], metricas['Distribuciones_Diferentes']],\n",
    "               color=['mediumseagreen', 'coral'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Comparaci√≥n de Distribuciones (Test KS)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('N√∫mero de Columnas', fontsize=11)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate([metricas['Distribuciones_Similares'], metricas['Distribuciones_Diferentes']]):\n",
    "    axes[1, 0].text(i, v + 0.001, str(v), ha='center', va='bottom', fontweight='bold') # Ajuste peque√±o en 'y'\n",
    "\n",
    "# Gr√°fico 4: Resumen de cambios\n",
    "categorias = ['Valores\\nNulos', 'Outliers', 'Columnas\\nModificadas']\n",
    "valores = [\n",
    "    metricas['Reduccion_Nulos'],\n",
    "    metricas['Reduccion_Outliers'],\n",
    "    metricas['Columnas_Modificadas']\n",
    "]\n",
    "colores = ['dodgerblue', 'orange', 'mediumpurple']\n",
    "\n",
    "axes[1, 1].bar(categorias, valores, color=colores, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Resumen de Cambios Aplicados', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Cantidad', fontsize=11)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(valores):\n",
    "    # Ajuste para que el texto aparezca correctamente si los valores son negativos\n",
    "    offset = max(valores) * 0.05\n",
    "    y_pos = v + offset if v >= 0 else v - offset\n",
    "    va = 'bottom' if v >= 0 else 'top'\n",
    "    axes[1, 1].text(i, y_pos, str(v), ha='center', va=va, fontweight='bold')\n",
    "\n",
    "\n",
    "# 3. T√çTULO GENERAL Y AJUSTE AUTOM√ÅTICO\n",
    "fig.suptitle('M√©tricas Globales de Comparaci√≥n: Original vs Cleaned',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Usamos el ajuste autom√°tico que es m√°s robusto despu√©s de un reinicio\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Preparaci√≥n para Alineaci√≥n de Datasets\n",
    "---\n",
    "\n",
    "Para hacer los datasets completamente comparables, necesitamos que tengan el mismo n√∫mero de filas.\n",
    "Vamos a crear una versi√≥n alineada del dataset cleaned que coincida con el original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ESTRATEGIA DE ALINEACI√ìN DE DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset Original: {len(df_original)} filas\")\n",
    "print(f\"Dataset Cleaned:  {len(df_cleaned)} filas\")\n",
    "print(f\"Diferencia:       {len(df_cleaned) - len(df_original)} filas extra en Cleaned\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPCIONES DE ALINEACI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "Para hacer los datasets comparables, podemos:\n",
    "\n",
    "1. **Usar las primeras N filas comunes**: Tomar solo las primeras 400 filas de ambos\n",
    "   ‚úÖ Ventaja: Mantiene el orden original\n",
    "   ‚ö†Ô∏è  Desventaja: Perdemos informaci√≥n de filas adicionales\n",
    "\n",
    "2. **Usar muestreo aleatorio**: Reducir Cleaned a 400 filas aleatorias\n",
    "   ‚úÖ Ventaja: M√°s representativo de todo el dataset\n",
    "   ‚ö†Ô∏è  Desventaja: Perdemos reproducibilidad sin semilla fija\n",
    "\n",
    "3. **Mantener Cleaned completo para an√°lisis**: Usar todo Cleaned para m√©tricas\n",
    "   ‚úÖ Ventaja: No perdemos informaci√≥n\n",
    "   ‚ö†Ô∏è  Desventaja: Comparaciones directas m√°s complejas\n",
    "\n",
    "Implementaremos la OPCI√ìN 1 para el an√°lisis comparativo directo.\n",
    "\"\"\")\n",
    "\n",
    "# Crear versiones alineadas\n",
    "n_rows_common = len(df_original)\n",
    "\n",
    "df_cleaned_aligned = df_cleaned.head(n_rows_common).copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset Cleaned Alineado creado: {len(df_cleaned_aligned)} filas\")\n",
    "print(f\"‚úÖ Ahora Original y Cleaned Alineado tienen el mismo tama√±o: {len(df_original)} filas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Comparaci√≥n Detallada con Datasets Alineados\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis celda por celda con datasets alineados\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISIS DETALLADO CELDA POR CELDA (Datasets Alineados)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "diferencias_detalladas = {}\n",
    "total_celdas = len(df_original) * len(columnas_numericas)\n",
    "celdas_modificadas = 0\n",
    "\n",
    "for col in columnas_numericas:\n",
    "    valores_original = df_original[col].values\n",
    "    valores_cleaned = df_cleaned_aligned[col].values\n",
    "    \n",
    "    # Calcular diferencias\n",
    "    diff = np.abs(valores_original - valores_cleaned)\n",
    "    n_cambios = np.sum(diff > 1e-10)  # Tolerancia num√©rica\n",
    "    celdas_modificadas += n_cambios\n",
    "    \n",
    "    if n_cambios > 0:\n",
    "        diferencias_detalladas[col] = {\n",
    "            'N_Cambios': n_cambios,\n",
    "            'Cambios_%': (n_cambios / len(df_original)) * 100,\n",
    "            'Diff_Media': np.mean(diff[diff > 1e-10]),\n",
    "            'Diff_Mediana': np.median(diff[diff > 1e-10]),\n",
    "            'Diff_Max': np.max(diff),\n",
    "            'Diff_Std': np.std(diff[diff > 1e-10])\n",
    "        }\n",
    "\n",
    "df_diffs_detalladas = pd.DataFrame(diferencias_detalladas).T.sort_values(\n",
    "    by='N_Cambios', ascending=False\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Total de celdas en el dataset: {total_celdas:,}\")\n",
    "print(f\"üìä Celdas modificadas: {celdas_modificadas:,}\")\n",
    "print(f\"üìä Porcentaje de celdas modificadas: {(celdas_modificadas/total_celdas)*100:.2f}%\")\n",
    "\n",
    "if len(df_diffs_detalladas) > 0:\n",
    "    print(f\"\\nTop 20 columnas con m√°s cambios:\\n\")\n",
    "    display(df_diffs_detalladas.head(20))\n",
    "else:\n",
    "    print(\"\\n‚úÖ No se detectaron cambios significativos entre los datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de las columnas m√°s modificadas\n",
    "if len(df_diffs_detalladas) > 0:\n",
    "    top_changed = df_diffs_detalladas.head(12)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (col, row) in enumerate(top_changed.iterrows()):\n",
    "        if idx >= 12:\n",
    "            break\n",
    "            \n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Calcular diferencias\n",
    "        valores_original = df_original[col].values\n",
    "        valores_cleaned = df_cleaned_aligned[col].values\n",
    "        diff = valores_cleaned - valores_original\n",
    "        \n",
    "        # Histograma de diferencias\n",
    "        ax.hist(diff, bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Sin cambio')\n",
    "        ax.set_title(f'{col.replace(\"_\", \" \")}\\n{int(row[\"N_Cambios\"])} cambios ({row[\"Cambios_%\"]:.1f}%)', \n",
    "                     fontsize=9, fontweight='bold')\n",
    "        ax.set_xlabel('Diferencia (Cleaned - Original)', fontsize=8)\n",
    "        ax.set_ylabel('Frecuencia', fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend(fontsize=7)\n",
    "    \n",
    "    # Ocultar axes vac√≠os\n",
    "    for idx in range(len(top_changed), 12):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Distribuci√≥n de Diferencias en Top 12 Columnas M√°s Modificadas', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. An√°lisis de Correlaci√≥n Comparativo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular matrices de correlaci√≥n\n",
    "corr_original = df_original[columnas_numericas].corr()\n",
    "corr_cleaned = df_cleaned_aligned[columnas_numericas].corr()\n",
    "\n",
    "# Calcular diferencia en correlaciones\n",
    "corr_diff = np.abs(corr_cleaned - corr_original)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISIS DE CAMBIOS EN CORRELACIONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Encontrar las correlaciones que m√°s cambiaron\n",
    "cambios_corr = []\n",
    "for i in range(len(corr_diff)):\n",
    "    for j in range(i+1, len(corr_diff)):\n",
    "        cambios_corr.append({\n",
    "            'Feature_1': corr_diff.index[i],\n",
    "            'Feature_2': corr_diff.columns[j],\n",
    "            'Corr_Original': corr_original.iloc[i, j],\n",
    "            'Corr_Cleaned': corr_cleaned.iloc[i, j],\n",
    "            'Diferencia_Abs': corr_diff.iloc[i, j]\n",
    "        })\n",
    "\n",
    "df_cambios_corr = pd.DataFrame(cambios_corr).sort_values(\n",
    "    by='Diferencia_Abs', ascending=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTop 15 pares de features con mayor cambio en correlaci√≥n:\\n\")\n",
    "display(df_cambios_corr.head(15))\n",
    "\n",
    "print(f\"\\nüìä Cambio promedio en correlaciones: {corr_diff.values[np.triu_indices_from(corr_diff, k=1)].mean():.4f}\")\n",
    "print(f\"üìä Cambio m√°ximo en correlaci√≥n: {corr_diff.values.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de diferencias en correlaciones\n",
    "# Seleccionar un subset de features para visualizaci√≥n m√°s clara\n",
    "features_subset = columnas_numericas[:15] if len(columnas_numericas) > 15 else columnas_numericas\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
    "\n",
    "# Matriz de correlaci√≥n Original\n",
    "sns.heatmap(corr_original.loc[features_subset, features_subset], \n",
    "            annot=False, cmap='coolwarm', center=0, \n",
    "            vmin=-1, vmax=1, ax=axes[0], cbar_kws={'label': 'Correlaci√≥n'})\n",
    "axes[0].set_title('Matriz de Correlaci√≥n - Original', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Matriz de correlaci√≥n Cleaned\n",
    "sns.heatmap(corr_cleaned.loc[features_subset, features_subset], \n",
    "            annot=False, cmap='coolwarm', center=0, \n",
    "            vmin=-1, vmax=1, ax=axes[1], cbar_kws={'label': 'Correlaci√≥n'})\n",
    "axes[1].set_title('Matriz de Correlaci√≥n - Cleaned', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Diferencia absoluta\n",
    "sns.heatmap(corr_diff.loc[features_subset, features_subset], \n",
    "            annot=False, cmap='Reds', \n",
    "            vmin=0, vmax=0.1, ax=axes[2], cbar_kws={'label': 'Diferencia Abs'})\n",
    "axes[2].set_title('Diferencia Absoluta en Correlaciones', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Comparaci√≥n de Matrices de Correlaci√≥n (Subset de 15 Features)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Exportaci√≥n de Datasets para DVC\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar ruta porque el notebook est√° en notebooks/\n",
    "output_dir = '../data/processed'  # ‚Üê CAMBIO: subir un nivel\n",
    "\n",
    "# Verificar que existe (por si acaso)\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset original (copia)\n",
    "df_original.to_csv(f'{output_dir}/turkish_music_emotion_v1_original.csv', index=False)\n",
    "print(f\"‚úÖ Exportado: turkish_music_emotion_v1_original.csv ({len(df_original)} filas)\")\n",
    "\n",
    "# Dataset cleaned alineado (mismo n√∫mero de filas que original)\n",
    "df_cleaned_aligned.to_csv(f'{output_dir}/turkish_music_emotion_v2_cleaned_aligned.csv', index=False)\n",
    "print(f\"‚úÖ Exportado: turkish_music_emotion_v2_cleaned_aligned.csv ({len(df_cleaned_aligned)} filas)\")\n",
    "\n",
    "# Dataset cleaned completo (con todas las filas)\n",
    "df_cleaned.to_csv(f'{output_dir}/turkish_music_emotion_v2_cleaned_full.csv', index=False)\n",
    "print(f\"‚úÖ Exportado: turkish_music_emotion_v2_cleaned_full.csv ({len(df_cleaned)} filas)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ARCHIVOS PREPARADOS PARA DVC\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "‚úÖ Se han creado 3 versiones del dataset en {Path(output_dir).resolve()}:\n",
    "\n",
    "1. **v1_original**: Dataset original sin modificaciones ({len(df_original)} filas)\n",
    "   - Para establecer baseline de rendimiento\n",
    "   - Referencia para comparaciones\n",
    "\n",
    "2. **v2_cleaned_aligned**: Dataset limpio alineado con original ({len(df_cleaned_aligned)} filas)\n",
    "   - Mismo tama√±o que original para comparaciones directas\n",
    "   - Incluye todas las transformaciones de limpieza\n",
    "\n",
    "3. **v2_cleaned_full**: Dataset limpio completo ({len(df_cleaned)} filas)\n",
    "   - Incluye todas las filas procesadas\n",
    "   - Para an√°lisis y modelado final\n",
    "\n",
    "üì¶ Pr√≥ximo paso: Versionar con DVC\n",
    "   Ejecuta en terminal:\n",
    "   1. cd ..  # Volver a la ra√≠z del proyecto\n",
    "   2. dvc add data\n",
    "   3. dvc push\n",
    "   4. git add data.dvc .gitignore\n",
    "   5. git commit -m 'feat: add turkish dataset versions v1 and v2'\n",
    "   6. git push\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERSIONAMIENTO AUTOM√ÅTICO CON DVC + GIT\n",
    "# ============================================\n",
    "print(\"=\" * 80)\n",
    "print(\"üöÄ INICIANDO VERSIONAMIENTO DE DATOS PROCESADOS CON DVC Y GIT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Encontrar el directorio correcto del proyecto\n",
    "current_dir = Path.cwd()\n",
    "print(f\"üìÅ Directorio inicial: {current_dir}\")\n",
    "\n",
    "# Buscar el directorio con .dvc/ (subir hasta encontrarlo)\n",
    "if not (current_dir / \".dvc\").exists():\n",
    "    # Si estamos en notebooks/, subir un nivel\n",
    "    if current_dir.name == \"notebooks\":\n",
    "        os.chdir(current_dir.parent)\n",
    "        current_dir = Path.cwd()\n",
    "        print(f\"‚úÖ Subido desde notebooks/ a: {current_dir}\")\n",
    "    else:\n",
    "        # Buscar en otras ubicaciones\n",
    "        potential_dirs = [\n",
    "            current_dir.parent,  # ‚Üê AGREGADO: primero intentar subir un nivel\n",
    "            current_dir / \"MLOps_Team24\",\n",
    "            current_dir.parent / \"MLOps_Team24\",\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for potential_dir in potential_dirs:\n",
    "            if potential_dir.exists() and (potential_dir / \".dvc\").exists():\n",
    "                os.chdir(potential_dir)\n",
    "                current_dir = Path.cwd()\n",
    "                print(f\"‚úÖ Cambiado a: {current_dir}\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print(\"\\n‚ùå ERROR: No se encontr√≥ el repositorio DVC\")\n",
    "            raise RuntimeError(\"No est√°s en un repositorio DVC\")\n",
    "else:\n",
    "    print(f\"‚úÖ Repositorio DVC detectado en: {current_dir}\")\n",
    "\n",
    "# 2. Crear mensaje de commit din√°mico\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "commit_message = f\"feat: add turkish dataset versions v1 and v2 - {timestamp}\"  # ‚Üê Mensaje m√°s espec√≠fico\n",
    "print(f\"üí¨ Mensaje de commit: {commit_message}\")\n",
    "\n",
    "# 3. Ejecutar comandos uno por uno\n",
    "commands = [\n",
    "    (\"dvc add data\", \"üì¶ Trackeando cambios en data/ con DVC...\"),\n",
    "    (\"dvc push\", \"‚òÅÔ∏è  Subiendo a S3...\"),\n",
    "    (\"git add data.dvc .gitignore\", \"üìù Agregando metadatos a Git...\"),\n",
    "    (f'git commit -m \"{commit_message}\"', \"üíæ Commiteando cambios...\"),\n",
    "    (\"git push\", \"üöÄ Subiendo a GitHub...\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "success = True\n",
    "no_changes = False\n",
    "\n",
    "for cmd, description in commands:\n",
    "    print(f\"\\n{description}\")\n",
    "    print(f\"   Ejecutando: {cmd}\")\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        shell=True,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=str(current_dir)\n",
    "    )\n",
    "    \n",
    "    # Mostrar output (stdout y stderr combinados)\n",
    "    output = (result.stdout + result.stderr).strip()\n",
    "    if output:\n",
    "        for line in output.split('\\n'):\n",
    "            if line.strip():\n",
    "                print(f\"   {line}\")\n",
    "    \n",
    "    # Verificar errores\n",
    "    if result.returncode != 0:\n",
    "        error_text = result.stderr.lower() + result.stdout.lower()\n",
    "        \n",
    "        # Casos NO cr√≠ticos (normales)\n",
    "        if \"nothing to commit\" in error_text or \"working tree clean\" in error_text:\n",
    "            print(\"   ‚ÑπÔ∏è  No hay cambios nuevos para commitear\")\n",
    "            no_changes = True\n",
    "            continue  # Continuar con siguiente comando\n",
    "        elif \"everything is up to date\" in error_text:\n",
    "            print(\"   ‚ÑπÔ∏è  Todo ya est√° actualizado\")\n",
    "            continue\n",
    "        elif \"no changes\" in error_text:\n",
    "            print(\"   ‚ÑπÔ∏è  No hay cambios en los datos\")\n",
    "            no_changes = True\n",
    "            continue\n",
    "        else:\n",
    "            # Error REAL\n",
    "            print(f\"   ‚ùå Error cr√≠tico: {result.stderr.strip()}\")\n",
    "            success = False\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if success:\n",
    "    if no_changes:\n",
    "        print(\"‚úÖ PROCESO COMPLETADO - NO HAB√çA CAMBIOS\")\n",
    "        print(\"\\nüìä Estado:\")\n",
    "        print(\"   ‚Ä¢ Datos ya estaban trackeados con DVC\")\n",
    "        print(\"   ‚Ä¢ Respaldo ya estaba en S3\")\n",
    "        print(\"   ‚Ä¢ No hab√≠a cambios que commitear\")\n",
    "        print(\"   ‚Ä¢ Todo sincronizado correctamente\")\n",
    "    else:\n",
    "        print(\"‚úÖ PROCESO DE VERSIONAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "        print(\"\\nüìä Resumen:\")\n",
    "        print(\"   ‚Ä¢ 3 versiones del dataset turkish a√±adidas\")\n",
    "        print(\"   ‚Ä¢ Datos trackeados con DVC\")\n",
    "        print(\"   ‚Ä¢ Respaldo en S3\")\n",
    "        print(\"   ‚Ä¢ Metadatos en GitHub\")\n",
    "        print(\"   ‚Ä¢ Cambios sincronizados con el equipo\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  HUBO UN ERROR DURANTE EL PROCESO\")\n",
    "    print(\"\\nüí° Soluci√≥n manual en terminal:\")\n",
    "    print(f\"   cd {current_dir}\")\n",
    "    print(\"   dvc add data\")\n",
    "    print(\"   dvc push\")\n",
    "    print(\"   git add data.dvc .gitignore\")\n",
    "    print(f'   git commit -m \"{commit_message}\"')\n",
    "    print(\"   git push\")\n",
    "    \n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Reporte Final de Comparaci√≥n\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar autom√°ticamente d√≥nde estamos\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Si estamos en notebooks/, subir un nivel. Si ya estamos en la ra√≠z, quedarse ah√≠\n",
    "if current_dir.name == 'notebooks':\n",
    "    reports_dir = current_dir.parent / 'reports'\n",
    "else:\n",
    "    # Ya estamos en la ra√≠z del proyecto\n",
    "    reports_dir = current_dir / 'reports'\n",
    "\n",
    "# Crear directorio si no existe\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REPORTE FINAL DE COMPARACI√ìN\")\n",
    "print(\"Turkish Music Emotion Dataset: Original vs Cleaned\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "reporte = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                         RESUMEN EJECUTIVO                                    ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "1. TAMA√ëO DEL DATASET\n",
    "   ‚Ä¢ Original:           {len(df_original):4d} filas √ó {df_original.shape[1]} columnas\n",
    "   ‚Ä¢ Cleaned:            {len(df_cleaned):4d} filas √ó {df_cleaned.shape[1]} columnas\n",
    "   ‚Ä¢ Diferencia:         {len(df_cleaned) - len(df_original):+4d} filas ({((len(df_cleaned) - len(df_original))/len(df_original)*100):+.2f}%)\n",
    "\n",
    "2. CALIDAD DE DATOS\n",
    "   ‚Ä¢ Valores nulos (Original):    {df_original.isnull().sum().sum():6d}\n",
    "   ‚Ä¢ Valores nulos (Cleaned):     {df_cleaned.isnull().sum().sum():6d}\n",
    "   ‚Ä¢ Reducci√≥n de nulos:          {df_original.isnull().sum().sum() - df_cleaned.isnull().sum().sum():6d}\n",
    "   \n",
    "   ‚Ä¢ Duplicados (Original):       {duplicados_original:6d}\n",
    "   ‚Ä¢ Duplicados (Cleaned):        {duplicados_cleaned:6d}\n",
    "\n",
    "3. OUTLIERS\n",
    "   ‚Ä¢ Outliers detectados (Original): {sum(outliers_original.values()):6d}\n",
    "   ‚Ä¢ Outliers detectados (Cleaned):  {sum(outliers_cleaned.values()):6d}\n",
    "   ‚Ä¢ Reducci√≥n de outliers:          {sum(outliers_original.values()) - sum(outliers_cleaned.values()):6d} ({((sum(outliers_original.values()) - sum(outliers_cleaned.values()))/max(sum(outliers_original.values()), 1)*100):.1f}%)\n",
    "\n",
    "4. CAMBIOS EN LOS DATOS (Datasets Alineados)\n",
    "   ‚Ä¢ Total de celdas:                {total_celdas:,}\n",
    "   ‚Ä¢ Celdas modificadas:             {celdas_modificadas:,}\n",
    "   ‚Ä¢ Porcentaje modificado:          {(celdas_modificadas/total_celdas)*100:.2f}%\n",
    "   ‚Ä¢ Columnas con cambios:           {len(df_diffs_detalladas)} de {len(columnas_numericas)}\n",
    "\n",
    "5. DISTRIBUCIONES ESTAD√çSTICAS\n",
    "   ‚Ä¢ Distribuciones similares (KS test, p>0.05):  {len(similares):3d} ({(len(similares)/len(columnas_numericas)*100):.1f}%)\n",
    "   ‚Ä¢ Distribuciones diferentes (KS test, p‚â§0.05): {len(diferentes):3d} ({(len(diferentes)/len(columnas_numericas)*100):.1f}%)\n",
    "\n",
    "6. DISTRIBUCI√ìN DE CLASES\n",
    "\"\"\"\n",
    "\n",
    "for clase in dist_original.index:\n",
    "    orig = dist_original[clase]\n",
    "    clean = dist_cleaned[clase]\n",
    "    diff = clean - orig\n",
    "    reporte += f\"   ‚Ä¢ {clase:8s}: Original={orig:3d}, Cleaned={clean:3d}, Diff={diff:+3d}\\n\"\n",
    "\n",
    "reporte += f\"\"\"\n",
    "7. CORRELACIONES\n",
    "   ‚Ä¢ Cambio promedio en correlaciones:    {corr_diff.values[np.triu_indices_from(corr_diff, k=1)].mean():.6f}\n",
    "   ‚Ä¢ Cambio m√°ximo en correlaci√≥n:        {corr_diff.values.max():.6f}\n",
    "\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                         CONCLUSIONES                                         ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "‚úÖ El proceso de limpieza ha resultado en:\n",
    "\n",
    "1. MEJORAS EN CALIDAD:\n",
    "   ‚Ä¢ {'Eliminaci√≥n total de valores nulos' if df_cleaned.isnull().sum().sum() == 0 else f'Reducci√≥n de {df_original.isnull().sum().sum() - df_cleaned.isnull().sum().sum()} valores nulos'}\n",
    "   ‚Ä¢ Reducci√≥n de {sum(outliers_original.values()) - sum(outliers_cleaned.values())} outliers\n",
    "   ‚Ä¢ Tratamiento de {len(df_diffs_detalladas)} columnas con valores modificados\n",
    "\n",
    "2. PRESERVACI√ìN DE ESTRUCTURA:\n",
    "   ‚Ä¢ {len(similares)} de {len(columnas_numericas)} columnas mantienen distribuciones similares\n",
    "   ‚Ä¢ Las correlaciones entre features se mantienen estables\n",
    "   ‚Ä¢ La distribuci√≥n de clases se preserva adecuadamente\n",
    "\n",
    "3. MODIFICACIONES APLICADAS:\n",
    "   ‚Ä¢ {(celdas_modificadas/total_celdas)*100:.2f}% de las celdas fueron modificadas\n",
    "   ‚Ä¢ Los cambios se concentran en {len(df_diffs_detalladas)} features espec√≠ficas\n",
    "   ‚Ä¢ Las transformaciones son consistentes y documentadas\n",
    "\n",
    "4. DATASETS PREPARADOS PARA DVC:\n",
    "   ‚úì v1_original: Baseline sin modificaciones\n",
    "   ‚úì v2_cleaned_aligned: Version limpia comparable (mismo tama√±o)\n",
    "   ‚úì v2_cleaned_full: Versi√≥n limpia completa para modelado\n",
    "\n",
    "üí° RECOMENDACIONES:\n",
    "   ‚Ä¢ Usar v2_cleaned_full para entrenamiento de modelos\n",
    "   ‚Ä¢ Mantener v1_original como baseline de referencia\n",
    "   ‚Ä¢ Versionar ambos datasets con DVC para trazabilidad\n",
    "   ‚Ä¢ Documentar todas las transformaciones en el pipeline\n",
    "\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "print(reporte)\n",
    "\n",
    "# Guardar reporte en la carpeta correcta de MLOps\n",
    "report_path = reports_dir / 'turkish_dataset_comparison_report.txt'\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(reporte)\n",
    "\n",
    "print(f\"\\n‚úÖ Reporte guardado en: {report_path.resolve()}\")\n",
    "print(f\"üìä Ubicaci√≥n MLOps: reports/turkish_dataset_comparison_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Gu√≠a para Configuraci√≥n de DVC\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Resumen Final\n",
    "---\n",
    "\n",
    "### ‚úÖ An√°lisis Completado\n",
    "\n",
    "Este notebook ha realizado una comparaci√≥n exhaustiva entre el dataset original y el dataset limpio:\n",
    "\n",
    "1. **An√°lisis Estructural**: Comparaci√≥n de dimensiones, tipos de datos y estructura general\n",
    "2. **Calidad de Datos**: Evaluaci√≥n de valores nulos, duplicados y outliers\n",
    "3. **Distribuciones**: An√°lisis de clases y distribuciones estad√≠sticas\n",
    "4. **Diferencias Detalladas**: An√°lisis celda por celda de las transformaciones\n",
    "5. **Tests Estad√≠sticos**: Validaci√≥n de similitud de distribuciones (Kolmogorov-Smirnov)\n",
    "6. **Correlaciones**: Comparaci√≥n de relaciones entre features\n",
    "7. **M√©tricas Globales**: Resumen cuantitativo de todos los cambios\n",
    "\n",
    "### üì¶ Archivos Generados\n",
    "\n",
    "- `turkish_music_emotion_v1_original.csv` - Dataset original (baseline)\n",
    "- `turkish_music_emotion_v2_cleaned_aligned.csv` - Dataset limpio alineado (comparaci√≥n directa)\n",
    "- `turkish_music_emotion_v2_cleaned_full.csv` - Dataset limpio completo (para modelado)\n",
    "- `comparison_report.txt` - Reporte detallado de comparaci√≥n\n",
    "- `dvc_setup_guide.sh` - Gu√≠a completa para configurar DVC\n",
    "\n",
    "### üöÄ Pr√≥ximos Pasos\n",
    "\n",
    "1. Configurar DVC usando la gu√≠a proporcionada\n",
    "2. Versionar los datasets generados\n",
    "3. Crear pipelines de experimentaci√≥n\n",
    "4. Entrenar modelos con diferentes versiones del dataset\n",
    "5. Comparar resultados de modelos entre versiones\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv Equipo24)",
   "language": "python",
   "name": "equipo24-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
