#!/usr/bin/env python3
"""
ğŸ” VALIDACIÃ“N FINAL - MLOps Team 24
Verifica que todo estÃ© correcto antes de entrega
"""

import sys
import subprocess
from pathlib import Path
import mlflow
from mlflow.tracking import MlflowClient
import pandas as pd
import warnings

# Suprimir warnings cosmÃ©ticos
warnings.filterwarnings('ignore')

def print_section(title):
    """Print formatted section header"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70)

def check_mark(condition, message):
    """Print check mark based on condition"""
    symbol = "âœ…" if condition else "âŒ"
    status = "OK" if condition else "FAIL"
    print(f"{symbol} [{status}] {message}")
    return condition

def main():
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘  ğŸ” VALIDACIÃ“N FINAL - MLOps Team 24                                â•‘")
    print("â•‘  Turkish Music Emotion Recognition - Fase 2                         â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    
    all_checks_passed = True
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 1. ESTRUCTURA DEL PROYECTO
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    print_section("1ï¸âƒ£  ESTRUCTURA DEL PROYECTO")
    
    required_files = [
        "README.md",
        "pyproject.toml",
        "requirements.txt",
        "acoustic_ml/__init__.py",
        "acoustic_ml/config.py",
        "acoustic_ml/dataset.py",
        "acoustic_ml/features.py",
        "acoustic_ml/modeling/sklearn_pipeline.py",
        "acoustic_ml/modeling/train.py",
        "scripts/training/run_mlflow_experiments.py",
        "data.dvc",
    ]
    
    for file in required_files:
        exists = Path(file).exists()
        all_checks_passed &= check_mark(exists, f"{file}")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 2. MÃ“DULO ACOUSTIC_ML
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    print_section("2ï¸âƒ£  MÃ“DULO ACOUSTIC_ML")
    
    try:
        from acoustic_ml.dataset import DatasetManager
        all_checks_passed &= check_mark(True, "Import DatasetManager")
    except Exception as e:
        all_checks_passed &= check_mark(False, f"Import DatasetManager: {e}")
    
    try:
        from acoustic_ml.features import create_full_pipeline
        all_checks_passed &= check_mark(True, "Import create_full_pipeline")
    except Exception as e:
        all_checks_passed &= check_mark(False, f"Import create_full_pipeline: {e}")
    
    try:
        from acoustic_ml.modeling.sklearn_pipeline import create_sklearn_pipeline
        all_checks_passed &= check_mark(True, "Import create_sklearn_pipeline")
    except Exception as e:
        all_checks_passed &= check_mark(False, f"Import create_sklearn_pipeline: {e}")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 3. DATOS Y DVC
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    print_section("3ï¸âƒ£  DATOS Y DVC")
    
    # Verificar data files
    data_files = [
        "data/processed/X_train.csv",
        "data/processed/X_test.csv",
        "data/processed/y_train.csv",
        "data/processed/y_test.csv",
    ]
    
    for file in data_files:
        exists = Path(file).exists()
        all_checks_passed &= check_mark(exists, f"{file}")
    
    # Verificar DVC status
    try:
        result = subprocess.run(
            ["dvc", "status"],
            capture_output=True,
            text=True,
            timeout=10
        )
        # DVC retorna "Data and pipelines are up to date." cuando estÃ¡ OK
        dvc_clean = (len(result.stdout.strip()) == 0 or 
                     "up to date" in result.stdout.lower())
        all_checks_passed &= check_mark(dvc_clean, "DVC status limpio")
        if not dvc_clean:
            print(f"   âš ï¸  DVC output: {result.stdout[:100]}")
    except Exception as e:
        all_checks_passed &= check_mark(False, f"DVC status check: {e}")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 4. PIPELINE SKLEARN
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    print_section("4ï¸âƒ£  PIPELINE SKLEARN")
    
    try:
        from acoustic_ml.modeling.sklearn_pipeline import create_sklearn_pipeline
        from acoustic_ml.dataset import DatasetManager
        
        # Cargar datos usando DatasetManager (mejor prÃ¡ctica MLOps)
        print("   ğŸ“‚ Cargando datos con DatasetManager...")
        dm = DatasetManager()
        X_train, X_test, y_train, y_test = dm.get_train_test_split()
        all_checks_passed &= check_mark(True, f"Datos cargados: {len(X_train)} train, {len(X_test)} test")
        
        # Crear pipeline
        print("   ğŸ”§ Creando pipeline...")
        pipeline = create_sklearn_pipeline(
            model_type="random_forest",
            model_params={"n_estimators": 10, "random_state": 42},
            scale_method="robust"
        )
        all_checks_passed &= check_mark(True, "Pipeline creado con scale_method='robust'")
        
        # Entrenar
        print("   ğŸ“ Entrenando pipeline...")
        pipeline.fit(X_train, y_train)
        all_checks_passed &= check_mark(True, "Pipeline entrenado")
        
        # Predecir
        print("   ğŸ”® Haciendo predicciones...")
        predictions = pipeline.predict(X_test)
        all_checks_passed &= check_mark(len(predictions) == len(X_test), "Predicciones generadas")
        
        # Score
        print("   ğŸ“Š Calculando accuracy...")
        score = pipeline.score(X_test, y_test)
        all_checks_passed &= check_mark(score > 0.5, f"Accuracy: {score:.4f}")
        
        print(f"   âœ… Pipeline funcional - Accuracy: {score:.4f}")
        
    except Exception as e:
        all_checks_passed &= check_mark(False, f"Pipeline test: {str(e)[:100]}")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 5. MLFLOW EXPERIMENTS
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    print_section("5ï¸âƒ£  MLFLOW EXPERIMENTS")
    
    try:
        # Configurar MLflow
        mlflow.set_tracking_uri("./mlruns")
        client = MlflowClient()
        
        # Buscar experimento
        experiment = mlflow.get_experiment_by_name("turkish-music-emotion-recognition")
        
        if experiment:
            all_checks_passed &= check_mark(True, f"Experimento encontrado: {experiment.name}")
            
            # Obtener runs
            runs = mlflow.search_runs(
                experiment_ids=[experiment.experiment_id],
                order_by=["start_time DESC"],
                max_results=20
            )
            
            num_runs = len(runs)
            all_checks_passed &= check_mark(num_runs >= 7, f"NÃºmero de runs: {num_runs} (esperado: â‰¥7)")
            
            if num_runs > 0:
                print(f"\n   ğŸ“Š Ãšltimos {min(10, num_runs)} experimentos:")
                print("   " + "-"*66)
                
                for idx, run in runs.head(10).iterrows():
                    run_name = run.get('tags.mlflow.runName', 'N/A')
                    test_acc = run.get('metrics.test_accuracy', 0)
                    
                    # Verificar que tiene mÃ©tricas
                    has_metrics = test_acc > 0
                    symbol = "âœ…" if has_metrics else "âš ï¸"
                    
                    print(f"   {symbol} {run_name[:35]:35s} | Acc: {test_acc:.4f}")
                
                # Verificar mejor modelo
                best_run = runs.loc[runs['metrics.test_accuracy'].idxmax()]
                best_name = best_run.get('tags.mlflow.runName', 'N/A')
                best_acc = best_run.get('metrics.test_accuracy', 0)
                
                print("\n   ğŸ† Mejor Modelo:")
                print(f"      Nombre: {best_name}")
                print(f"      Test Accuracy: {best_acc:.4f}")
                
                all_checks_passed &= check_mark(
                    best_acc >= 0.75, 
                    f"Mejor accuracy â‰¥ 75%: {best_acc:.2%}"
                )
                
                # Verificar artefactos del mejor modelo
                best_run_id = best_run['run_id']
                artifacts = client.list_artifacts(best_run_id)
                artifact_names = [a.path for a in artifacts]
                
                has_model = any('model' in name.lower() for name in artifact_names)
                has_artifacts = len(artifact_names) > 0
                
                # Si tiene artefactos pero no modelo especÃ­ficamente, es warning no error
                if has_model:
                    check_mark(True, "Modelo guardado en artefactos")
                elif has_artifacts:
                    print(f"   âš ï¸  [INFO] Artefactos encontrados: {', '.join(artifact_names[:3])}")
                    print(f"   â„¹ï¸  No se encontrÃ³ carpeta 'model' pero hay {len(artifact_names)} artefactos")
                else:
                    all_checks_passed &= check_mark(False, "No hay artefactos guardados")
                
                # Verificar que NO hay warnings (chequeando los logs)
                print("\n   ğŸ” Verificando warnings en MLflow...")
                # Esto es informativo, no afecta el check
                print("   â„¹ï¸  Ejecuta experimentos y verifica que no haya warnings en consola")
                
        else:
            all_checks_passed &= check_mark(False, "Experimento 'turkish-music-emotion-recognition' no encontrado")
            print("   âš ï¸  Ejecuta: python scripts/training/run_mlflow_experiments.py")
    
    except Exception as e:
        all_checks_passed &= check_mark(False, f"MLflow check: {str(e)[:100]}")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 6. GIT Y GITHUB
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    print_section("6ï¸âƒ£  GIT Y GITHUB")
    
    try:
        # Git status
        result = subprocess.run(
            ["git", "status", "--porcelain"],
            capture_output=True,
            text=True,
            timeout=5
        )
        git_clean = len(result.stdout.strip()) == 0
        all_checks_passed &= check_mark(git_clean, "Git working tree limpio")
        if not git_clean:
            print("   âš ï¸  Archivos sin commit:")
            for line in result.stdout.strip().split('\n')[:5]:
                print(f"      {line}")
        
        # Verificar remote
        result = subprocess.run(
            ["git", "remote", "-v"],
            capture_output=True,
            text=True,
            timeout=5
        )
        has_remote = "origin" in result.stdout
        all_checks_passed &= check_mark(has_remote, "Git remote configurado")
        
        # Ãšltimos commits
        result = subprocess.run(
            ["git", "log", "--oneline", "-3"],
            capture_output=True,
            text=True,
            timeout=5
        )
        print("\n   ğŸ“ Ãšltimos 3 commits:")
        for line in result.stdout.strip().split('\n'):
            print(f"      {line}")
        
    except Exception as e:
        all_checks_passed &= check_mark(False, f"Git check: {e}")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # 7. TESTS
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    print_section("7ï¸âƒ£  TESTS")
    
    test_scripts = [
        "scripts/validate_dataset.py",
        "scripts/validate_features.py",
        "scripts/validate_plots.py",
    ]
    
    for script in test_scripts:
        if Path(script).exists():
            try:
                result = subprocess.run(
                    ["python", script],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                passed = result.returncode == 0
                script_name = Path(script).name
                all_checks_passed &= check_mark(passed, f"{script_name}")
                if not passed and result.stderr:
                    print(f"   âš ï¸  Error: {result.stderr[:100]}")
            except Exception as e:
                all_checks_passed &= check_mark(False, f"{script}: {e}")
        else:
            print(f"   â­ï¸  {script} no encontrado")
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # RESUMEN FINAL
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    print("\n" + "="*70)
    if all_checks_passed:
        print("  âœ… VALIDACIÃ“N COMPLETADA - TODO OK")
        print("  ğŸ‰ Proyecto listo para entrega")
    else:
        print("  âš ï¸  VALIDACIÃ“N COMPLETADA - ALGUNOS CHECKS FALLARON")
        print("  ğŸ”§ Revisa los items marcados con âŒ")
    print("="*70)
    
    print("\nğŸ¯ Siguiente paso:")
    print("   â†’ Abrir MLflow UI: mlflow ui --port 5001")
    print("   â†’ URL: http://localhost:5001")
    print("   â†’ Verificar visualmente que no hay warnings en los logs")
    print("\n")
    
    return 0 if all_checks_passed else 1

if __name__ == "__main__":
    sys.exit(main())
